---
title: "Problem Set 6"
author: "Daniel Shapiro"
date: "10/20/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(stats)
library(infer)
library(pwr)
```

### Question 1 Background:

*Download the \textit{votes.csv} dataset from the course website.  These data describe the number of votes obtained by the Democratic and Republican candidates in each presidential election from 1932 to 2008.*

```{r download}
votes <- read.csv("votes.csv")
```

### 1a) Delete observations for Alaska, DC, and Hawaii (since residents of these states did not have the vote throughout this entire time period). Convert the data from wide to long format, with four variables (state, party, election, and vote).

```{r 1a}

# I'm going to do this in kind of an annoying way. 

votes <- votes %>%
  filter(state != "Hawaii") %>%
  filter(state != "Alaska") %>%
  filter(state != "District of Columbia") %>%
  pivot_longer(!state, names_to = "election", values_to = "vote") %>%
  separate(election, c("party", "election"))

```

### 1b) Transform the data to a state-election year dataset, with four variables (election, state, pcDem, turnout).

```{r 1b}
# Had to suppress this really annoying message 

options(dplyr.summarise.inform = FALSE)

newvotes <- votes %>%
  group_by(election, state) %>%
  summarize(turnout = sum(vote))

demvotes <- votes %>%
  filter(party == "d")

votemerge <- merge(newvotes, demvotes, by = c("election", "state")) %>%
  select(-party) %>%
  mutate(pcDem = vote/turnout*100) %>%
  select(-vote)

votemerge <- votemerge[,c(1, 2, 4, 3)]
```

### 1c) Plot democratic vote share over time for Pennsylvania, South Carolina, and West Virginia using a \texttt{ggplot()} line graph with clear and appropriately labeled legends and axes.

```{r 1c}
# West Virginia is misspelled...edit out with recode() below

cdata <- votemerge %>%
  filter(state %in% c("Pennsylvania", "South Carolina", "West Vriginia")) 

cdata$state <- recode(cdata$state, `West Vriginia` = "West Virginia")

cdata$election <- as.numeric(cdata$election)

ggplot(cdata, aes(x = election, y = pcDem)) +
  geom_point() +
  geom_line() +
  aes(color = state) +
  labs(x = "Election", 
       y = "Percent Democratic", 
       color = "State", 
       title = "Democratic Vote Pct., 1932-2008",
       subtitle = "Data from Pennsylvania, South Carolina, West Virginia")
```

### 1d) Use \texttt{ggplot()} to make a scatterplot showing the relationship between 2004 and 2008 democratic vote share.

```{r 1d}
ddata <- votemerge %>%
  filter(election %in% c(2004, 2008))
```

```{r inset}
graphdata <- ddata %>% 
  select(-turnout) %>%
  pivot_wider(names_from = election, values_from = pcDem)
```

```{r dplot}
ggplot(graphdata, aes(x = `2004`, y = `2008`)) +
  geom_point() +
  labs(x = "2004 Vote Share (Pct)",
       y = "2008 Vote Share (Pct)",
       title = "Relationship between 2004 and 2008 Democratic Vote Shares")
```

### 1e) Use a paired t-test to test whether the difference between the 2004 and 2008 democratic vote share was significantly different across states. Interpret your results.

```{r 1e}

# First, I split my data into two -- 2004 and 2008 and select pcDem.

e2004 <- ddata %>%
  filter(election == 2004) %>%
  select(pcDem)

e2008 <- ddata %>%
  filter(election == 2008) %>%
  select(pcDem)

t.test(e2004[,1], e2008[,1], paired = TRUE, alternative = "two.sided")
```

The p-value of the test is $8.493^{-14}$, which is lower than the significance level $\alpha = 0.05$. Thus, we can reject the null hypothesis and say that the average Democratic vote share in 2008 is significantly different than the average Democratic vote share in 2004, with a p-value at $8.493^{-14}$.

### Question 2 Background:

*Download the \textit{olken.csv} dataset from the course website, which comes from an experiment described in Benjamin A. Olken. 2007.  ``Monitoring Corruption: Evidence from a Field Experiment in Indonesia.'' \textit{ Journal of Political Economy}.  115(2): 200-249.*

*Missing data from the dataset were omitted.  The experiment sought to test the effect of efforts to encourage communities to monitor road building projects in Indonesian villages on reducing corruption.  The main treatment variable is whether or not residents in a particular village were invited to participate at accountability meetings in which project officials account for how they spent project funds.  The main dependent variable is a measure of the difference between what the villages claimed they spent on road construction and an independent estimate of what the villages actually spent.  The variables in the dataset are:*

\begin{itemize}
\item \textit{pct\_missing}: Percent expenditures missing
\item \textit{treat\_invite}: Treatment assignment
\item \textit{mosques}: Mosques per 1,000
\end{itemize}

```{r 2setup}
olken <- read.csv("olken.csv")
```


### 2a) Estimate the population average $\mu$ of the \textit{pct\_missing} variable with the sample mean. Report the standard error and a 95\% confidence interval for your estimate as well.

```{r}
missing_mean <- mean(olken$pct_missing)
std_err <- sd(olken$pct_missing) / sqrt(length(olken$pct_missing))
confidence <- t.test(olken$pct_missing)$conf.int

missing_mean
std_err
confidence
```

### 2b) Conduct a two-sided t-test with the null hypothesis that $\mu_0 = 0$ with $\alpha = 0.05$.  Report your test statistic and p-value.  What is the interpretation of this p-value?  Can you reject the null hypothesis? 

```{r 2b}
t.test(olken$pct_missing)
```

Here, the test statistic is 14.904, and the p-value is $2.2*10^{-16}$. The p-value is way lower than 0.05, so the null hypothesis can be rejected at the 5% level of significance.

### 2c) Repeat b) with the null hypothesis $\mu_0 = 0.25$. Hint: check the documentation for \texttt{t.test} using \texttt{?t.test} to figure out how to do this.

```{r 2c}
t.test(olken$pct_missing, mu = 0.25)
```

Here, the test statistic is -0.86222, and the p-value is 0.389. This time, the p-value is way higher than 0.05, meaning that we cannot reject this null hypothesis at the 5% level of significance.

### 2d) Calculate the $t$-statistic of the null hypothesis in parts b) and c) analytically. Confirm you get the same results. Explain what the test statistic means. 

To get our test statistic, we can use the following formula: $\frac{\mu_a - \mu_0}{\hat{SE}(\mu_a)}$. In the first case, $\mu_0 = 0$, so we can simply say $\frac{\mu_a}{\hat{SE}(\mu_a)}$, or our missing_mean value over the std_err value from our previous question. 

```{r 2db}
tvalue <- missing_mean/std_err
tvalue
```

This is the same result as the t-value we got in 2b). 

For 2c), we can get our test statistic using the same formula, but instead, it turns into $\frac{\mu_a - .25}{\hat{SE}(\mu_a)}$. Here it is:

```{r 2dc}
tvaluec <- (missing_mean-.25)/std_err
tvaluec
```

Again, this confirms the value that we got in 2c).

The test statistic is any function of the data that dictates whether the null should be rejected or not. In this case, we are using the t-value, given that we're doing t-tests.

### 2e) Let $Y_t$ denote \textit{pct\_missing} for villages that received the treatment and $Y_c$ denote \textit{pct\_missing} for villages that did not receive treatment.  Assume that $Y_t$ and $Y_c$ are independent and have unequal variances. Conduct a two-sided t-test for the equality of means of $Y_t$ and $Y_c$ where the null hypothesis is $H_0: \mu_{Y_t} = \mu_{Y_c}$.  Report your test statistic and p-value, as well as a 95\% confidence interval. Can you reject the null hypothesis at the $\alpha = 0.05$ level?  Give a brief substantive explanation of your result (i.e.\ what can you say about the effect of the treatment on corruption?).

```{r 2e}
# First, I need to split these into two vectors: one that was treated and one
# that was not.

treated <- olken %>%
  filter(treat_invite == 1)

untreated <- olken %>%
  filter(treat_invite == 0)

# Now, I can insert these two vectors into a t-test.

t.test(treated$pct_missing, untreated$pct_missing)
```

Here, we used Welch's Two Sample t-test, which can also basically be described as an unequal variances t-test. The test statistic is the t-value, at -0.75376, and the p-value is 0.4515. The lower bound of the 95% confidence interval is -0.09, and the upper bound is 0.04. The p-value is quite high -- 0.4515, so we cannot reject the null hypothesis at the 5% level of significance.

Putting this into more applied language, basically, the null hypothesis is that we should observe *no* difference in the amount of money "missing" between villages that have been treated (i.e. where villagers have been invited to participate in accountability meetings) and villages that were untreated (where villagers were not invited to said meetings). If we cannot reject the null hypothesis at the 5% level of significance, it means that we cannot argue (at this level of significance) that inviting villagers to participate in accountability meetings had a significant impact on corrupt activity. This does not mean we can *confirm* the null hypothesis; simply that we cannot reject it.

### 2f) Suppose you were actually running this experiment and your research assistant comes to you and says "I think we can get better results if we ran a one-sided test instead."  Do you think this is a good idea?  Why or why not?  What assumptions would you be making?

I would say that it's most likely not a good idea. As Jane said in class, using a one-sided test "comes off as shady." There are a number of assumptions that you're making, most notably that the consequences of not recording an effect in the direction that is getting skipped are negligible. In this case, this could potentially look like the research assistant saying that he/she is only interested in seeing whether the treated villages experienced less corruption than the untreated villages, and that he/she does not care at all about any potential data in which the untreated villages experienced less corruption than the treated villages. 

This does not look like a great idea. The research question does not say that this is what the researchers are looking for; rather, it just says that the experiment tests "the effect of efforts." This "effect," as listed in the research question, is value-neutral. If I would even consider running a one-sided test instead of a normal two-sided one, I'd want to have a well-written explanation as to why exactly we only care about effects in one direction, and not the other. Given that I don't see a particularly good rationale why this would be the case in this instance, I don't think I'd be likely to accept my research assistant's proposition.

### 2g) Repeat e) using a one-sided test where the alternative hypothesis is $H_1: \mu_{Y_t} < \mu_{Y_c}$.

```{r 2g}
t.test(treated$pct_missing, untreated$pct_missing, alternative = "less")
```

Here, we see the test statistic (t-value here) at -0.75376 and the p-value at 0.2258. Given that we're essentially cutting the effect that we cared about in 2e) in half, it makes sense that we get a p-value that's about half of the p-value in 2e). Additionally, since we're only doing a one-sided test, the left bound of the confidence interval is $-\infty$ and the right bound is approximately 0.027. Again, we cannot reject the null hypothesis at the $\alpha = 0.05$ level; the p-value is too high. 

### 2h) Now let $Y_t$ and $Y_c$ denote the variable \textit{mosques} for treatment and control observations respectively.  Again assume independence and unequal variances.  Run a two-sided t-test for the equality of means with $H_0: \mu_{Y_t} = \mu_{Y_c}$.  Report your test statistic and p-value.  Can you reject the null hypothesis at the $\alpha=0.05$ level?  Does your result make sense intuitively in an experimental setting where the treatment variable was randomly assigned?  Why or why not?

```{r 2h}
t.test(treated$mosques, untreated$mosques)
```

In this t-test, the test statistic (t-value) is -0.66241, and the p-value is 0.5082. The p-value is quite high, so we cannot reject the null hypothesis at the 5% level of significance.

Intuitively, yes, this makes sense, and it is probably good that we do not see some very convincing data here one way or the other. Given that the treatment has nothing to do with a village's "mosques per 1,000" number, it would raise a red flag, in my opinion, if the treated population had significantly more or fewer mosques per capita than the untreated population. If this were the case, then it would be worth a look deeper into the experiment to determine how truly "random" the assignment of the treatment was. 